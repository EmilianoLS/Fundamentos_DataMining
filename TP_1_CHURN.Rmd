---
title: "Tp_churn_data_mining"
output: html_notebook
---

Importo las librerias que voy a usar

```{r message=FALSE, warning=FALSE}
library("stringr")
library("data.table")
library(ggplot2)
library(dplyr)
library(gridExtra)
library(reshape2)
library(stats)
library(corrplot)
library(class)
library(fastDummies)
library(rpart)
library(caret)
library(rpart.plot)
library(randomForest)
library(caTools)
library(robustbase)
library(xgboost)
```

Defino funcion para cargar todos los data frames de entrenamiento

```{r Load_csv_function, message=FALSE, warning=FALSE}
load_csv_data <- function(csv_file, sample_ratio = 1, drop_cols = NULL,
                          sel_cols = NULL) {
  # Esta funcion carga un unico csv con los parametros que le indique
  
  dt <- fread(csv_file, header = TRUE, sep = ",", stringsAsFactors = TRUE,
              na.strings = "", drop = drop_cols, select = sel_cols,
              showProgress = TRUE)
  return(dt)
}



load_train_data <- function(data_dir, train_file="/train_", sample_ratio=1,
                            drop_cols=NULL, sel_cols=NULL) {
  # Esta funcion se encarga de concatenar todos los dataframes juntos
  
  
  
  train_days <- seq(1, 5, by=1)
  
  dfs <- list()
  
  for (i in train_days){
  
    dfs[[i]] <- load_csv_data(csv_file = paste(data_dir,train_file,as.character(i),".csv", sep = ''), 
                              sample_ratio = sample_ratio, drop_cols = drop_cols, sel_cols = sel_cols)
  }
  
  # Uno todos los dataframes en uno solo
  
  df <- (rbindlist(dfs, fill=TRUE))
  
  # Reordeno las columnas alfabeticamente
  
  setcolorder(df, sort(colnames(df)))
  
  # Creo la columna Label con las condiciones de churn explicadas
  
  df[, Label := as.numeric(Label_max_played_dsi == 3)] 
  set.seed(123)
  if (sample_ratio < 1) {
    
    sample_size <- as.integer(sample_ratio * nrow(df))
    
    df <- df[sample(.N, sample_size)]
  }
  
  rm(dfs)
  
  
  return(df)
}
```

# Analisis exploratorio de las variables

En esta secciÛn se va a hacer una primera aproximaciÛn a los datos, observando las relaciones entre las variables, entendiendo su composiciÛn, cÛmo est·n distribuidas, etc.

Cargo dataframe en su totalidad

```{r}
csv_dir <- "C:/Users/elosasso/OneDrive - Universidad Torcuato Di Tella/Mineria de datos/datasets"

train <- load_train_data(csv_dir, sample_ratio = 1)
train$churn <- train$Label == 1
```

Hago una primera visualizaciÛn de la estructura para ver cÛmo se encuentran organizados los datos y para ver que todo estÈ cargado correctamente

```{r}
head((train));dim(train)
```

Visualizo cuantiles, mÌnimos, m·ximos y NAs

```{r}
summary(train)
```

A raÌz de la tabla anterior, ya podemos observar dos cosas muy importantes:

* La columna 'age' no puede tomarse para analizar por su alto contenido de NAs, no podemos inferir estos datos de ninguna forma

* La columna 'BuyCard_sum_dsi1' tiene valores negativos, esto pareciera ser un error, y viendo el comportamiento en las demas columnas de este estilo, podria imputarse el valor 0 a estos negativos

Reviso las columnas con valores nulos y sus porcentajes

Para eso creo una funcion que me trae el porcentaje de nulos en las columnas que unicamente tienen nulos

```{r find_nulls_function, message=FALSE, warning=FALSE}
find_nulls <- function(df){
    columnas_nas <- list()

    for (i in colnames(df)){
      nans <- nrow(filter(select(df,i), !complete.cases(select(df,i))))

      if (nans != 0){

        columnas_nas[[i]] <- (nans/nrow(df))*100
      }

    }
    return(columnas_nas)
    }

as.data.frame(find_nulls(train))
```

# VisualizaciÛn

En esta parte, empezamos a meternos m·s en los datos. Queremos entender, o al menos, tener un primer insight de las variables que m·s relaciÛn tienen con la target, si hay mayor o menor proporciÛn de alguna variable en particular.

La primera relaciÛn que se nos ocurriÛ fue con las variables relacionadas al tutorial. Cuando jugamos el juego, vimos que el tutorial es bastante extenso, y cubre bastantes de los elementos m·s importantes del juego. Suponemos que existe, entonces, alguna relaciÛn entre el hecho de haber terminado el tutorial y hacer churn o no.

```{r}
ggplot(train) + geom_bar(mapping = aes(x = TutorialFinish, fill = churn), position = 'fill') +
labs(x = 'Termino el tutorial', y = '% churn') +
ggtitle('Proporcion de churn segun termino o no el tutorial')
```

Vemos en el gr·fico de proporciones que, si bien la proporciÛn de personas que hicieron churn es mayor para el grupo que no terminÛ el tutorial, la diferencia no es significativa respecto a los que sÌ lo terminaron, sino que tienen niveles similares.


Por otro lado, supuonemos que una persona que pierda m·s batallas ser· m·s propensa a hacer Churn. 
Para comprobar esta hipÛtesis, sumamos todas las batallas iniciadas, y todas las batallas perdidas, y hacemos un gr·fico de relaciÛn, separado por los grupos de churn y no churn.


```{r}
train$sum_lost_battles <- train$LoseBattle_sum_dsi0 + train$LoseBattle_sum_dsi1 + train$LoseBattle_sum_dsi2 + train$LoseBattle_sum_dsi3
train$sum_win_battles <- train$WinBattle_sum_dsi0 + train$WinBattle_sum_dsi1 + train$WinBattle_sum_dsi2 + train$WinBattle_sum_dsi3
train$sum_start_battle <- train$StartBattle_sum_dsi0 + train$StartBattle_sum_dsi1 + train$StartBattle_sum_dsi2 + train$StartBattle_sum_dsi3

# Tomo una muestra del 10% del total para graficar los puntos porque sino es muy pesado

set.seed(123)
sample_size <- as.integer(0.1 * nrow(train))    
df <- train[sample(.N, sample_size)]

ggplot(filter(df, complete.cases(sum_start_battle))) +
geom_point(mapping = aes(x = sum_start_battle, y = sum_lost_battles, color = churn), alpha = 0.3) +
geom_smooth(mapping = aes(x = sum_start_battle, y = sum_lost_battles)) +
labs(x = 'Batallas iniciadas', y = 'Batallas perdidas') +
ggtitle('Batallas perdidas vs Batallas iniciadas')
```

Lo primero que vemos es que hay una relaciÛn casi lineal entre las batallas iniciadas y las perdidas. Esto es lÛgico, porque cuanto m·s se juegue, es lÛgico pensar que la cantidad de veces que pierda tambiÈn tienda a aumentar. 

Sin embargo, cuando vemos la distribuciÛn de los puntos de cada color, vemos que todos se encuentran bastante superpuestos, con lo cual, estas variables no separan casi nada los grupos de churn y no churn.

Otra idea, relacionada a las batallas iniciadas del punto anterior, es que puede llegar a ocurrir que exista una relaciÛn entre la cantidad de sesiones que inicia el usuario y el hecho de hacer churn. Por esto, vemos la cantidad de sesiones iniciadas para cada grupo, en cada uno de los dÌas desde la instalaciÛn.

Luego se calcula la media de sesiones iniciadas para entender si verdaderamente hay una relaciÛn.

```{r}
train_melt <- melt(select(train, user_id, StartSession_sum_dsi0,StartSession_sum_dsi1,StartSession_sum_dsi2,
                          StartSession_sum_dsi3, churn), id = c('user_id','churn'))

grafico <- train_melt %>% group_by(variable,churn) %>% summarize(total = sum(value)) %>% ungroup()
grafico

ggplot(grafico, mapping = aes(x = total, y = variable, fill = churn)) +
geom_bar(color = 'black',stat = 'identity', position = 'dodge') + facet_wrap(~churn, ncol = 1) +
labs(x = 'Total de sesiones', y = 'Dia') +
ggtitle('Cantidad de sesiones por dia')
```

Agrupo por dia desde la instalacion y calculo el intervalo de confianza para la media

```{r}
grafico_2 <- train_melt %>% group_by(variable,churn) %>%
summarize(intervalo_inf = t.test(value, conf.level = 0.95)$conf.int[1], 
         intervalo_sup = t.test(value, conf.level = 0.95)$conf.int[2],
         mean = mean(value)) %>% ungroup()

grafico_2

ggplot(grafico_2) +
geom_point(mapping = aes(x = mean, y = variable, fill = churn),color = 'black',stat = 'identity') + 
geom_errorbar(aes(xmin = intervalo_inf, xmax = intervalo_sup, y = variable), color = 'red') +
facet_wrap(~churn, ncol = 1) +
labs(x = 'Media de sesiones', y = 'Dia') + 
ggtitle('Cantidad de sesiones promedio por dia')
```

Vemos que encontramos alguna especie de relaciÛn entre las sesiones y los grupos de churn y no churn. Puntualmente se ve que el comportamiento en sesiones iniciadas para un grupo y otro es completamente diferente. 

Por un lado, el grupo de usuarios que hicieron churn tiene una media de sesiones iniciadas diarias m·s baja que el grupo que no hizo churn, y que posteriormente baja hasta el tercer dÌa de la instalaciÛn. Esto no pasa con el otro grupo, sino que m·s bien se mantiene constante durante los primeros dÌas


```{r}


train$install_day <- if_else(train$install_date %in% seq(from = 0, to = (56*7), by = 7), 'A', 
                             if_else(train$install_date %in% seq(from = 1, to = (56*7)+1, by = 7), 'B',
                                     if_else(train$install_date %in% seq(from = 2, to = (56*7)+2, by = 7), 'C',
                                             if_else(train$install_date %in% seq(from = 3, to = (56*7)+3, by = 7), 'D',
                                                     if_else(train$install_date %in% seq(from = 4, to = (56*7)+4, by = 7), 'F', 
                                                             if_else(train$install_date %in% seq(from = 5, to = (55*7)+5, by = 7), 'G',
                                                                     if_else(train$install_date %in% seq(from = 6, to = (55*7)+6, by = 7), 'H','NA')))))))
#train$install_day <- if_else(train$install_date+1 %in% c(1:56*7,by = 7))

```

```{r}
prop_churn_dia <- as.data.frame(prop.table(table(train$install_day, train$Label),2)[,2])

names(prop_churn_dia)[1] <- 'Proporcion_churn'
prop_churn_dia$day <- row.names(prop_churn_dia)
ggplot(prop_churn_dia, aes(day, Proporcion_churn)) + geom_point() +
  labs(x = 'Dia de instalacion', title = 'Proporcion de churn total segun el dia de instalacion')
rm(prop_churn_dia)
```




Busco si existe una relaci√≥n entre la cantidad de batallas ganadas/ batallas perdidas

```{r}
train$rt_lose_win_0 <- train$WinBattle_sum_dsi0 - train$LoseBattle_sum_dsi0
train$rt_lose_win_1 <- train$WinBattle_sum_dsi1 - train$LoseBattle_sum_dsi1
train$rt_lose_win_2 <- train$WinBattle_sum_dsi2 - train$LoseBattle_sum_dsi2
train$rt_lose_win_3 <- train$WinBattle_sum_dsi3 - train$LoseBattle_sum_dsi3

train_melt <- melt(select(train, user_id, rt_lose_win_0,rt_lose_win_1,rt_lose_win_2,
                          rt_lose_win_3, churn), id = c('user_id','churn'))

grafico_4 <- train_melt %>% group_by(variable,churn) %>%
summarize(intervalo_inf = t.test(value, conf.level = 0.95)$conf.int[1], 
         intervalo_sup = t.test(value, conf.level = 0.95)$conf.int[2],
         mean = mean(value)) %>% ungroup()

ggplot(grafico_4) +
geom_point(mapping = aes(x = mean, y = variable, fill = churn),color = 'black',stat = 'identity') + 
geom_errorbar(aes(xmin = intervalo_inf, xmax = intervalo_sup, y = variable), color = 'red') +
facet_wrap(~churn, ncol = 1) +
labs(x = 'Media de resultados por dia', y = 'Dia') + 
ggtitle('Resultados promedio por d√???a')
```

Promedio de batallas perdidas por d√???a

```{r}
train_melt <- melt(select(train, user_id, LoseBattle_sum_dsi0,LoseBattle_sum_dsi1,LoseBattle_sum_dsi2,
                          LoseBattle_sum_dsi3, churn), id = c('user_id','churn'))

grafico_5 <- train_melt %>% group_by(variable,churn) %>%
summarize(intervalo_inf = t.test(value, conf.level = 0.95)$conf.int[1], 
         intervalo_sup = t.test(value, conf.level = 0.95)$conf.int[2],
         mean = mean(value)) %>% ungroup()

ggplot(grafico_5) +
geom_point(mapping = aes(x = mean, y = variable, fill = churn),color = 'black',stat = 'identity') + 
geom_errorbar(aes(xmin = intervalo_inf, xmax = intervalo_sup, y = variable), color = 'red') +
facet_wrap(~churn, ncol = 1) +
labs(x = 'Media de batallas perdidas', y = 'Dia') + 
ggtitle('Batallas perdidas en promedio por d√???a')
```

Promedio de batallas iniciadas (esto lo hago para entender si la menor cantidad de batallas perdidas por aquellos que hicieron churn en el gr√°fico anterior se debe a que jugaron menos)

```{r}
train_melt <- melt(select(filter(train,complete.cases(StartBattle_sum_dsi1)), user_id,
                          StartBattle_sum_dsi0,StartBattle_sum_dsi1,StartBattle_sum_dsi2,
                          StartBattle_sum_dsi3, churn), id = c('user_id','churn'))

grafico_6 <- train_melt %>% group_by(variable,churn) %>%
summarize(intervalo_inf = t.test(value, conf.level = 0.95)$conf.int[1], 
         intervalo_sup = t.test(value, conf.level = 0.95)$conf.int[2],
         mean = mean(value)) %>% ungroup()

ggplot(grafico_6) +
geom_point(mapping = aes(x = mean, y = variable, fill = churn),color = 'black',stat = 'identity') + 
geom_errorbar(aes(xmin = intervalo_inf, xmax = intervalo_sup, y = variable), color = 'red') +
facet_wrap(~churn, ncol = 1) +
labs(x = 'Media de batallas inicidadas', y = 'Dia') + 
ggtitle('Batallas iniciadas en promedio por d√???a')
```

Feature Engineering
-------------------

Ahora que ya preparamos una analitica y comprendemos un poco mejor los datos, procedemos a preparar las variables para poder aplicarles un modelo.

Primero tengo que excluir los casos no confiables: Todos aquellos que tengan Label_max_played_dsi igual a 3, que hayan sido instalados en los dias que van de 383 a 395 inclusive, ya que no son validos (desconozco si la persona hizo churn o no).

Entreno con un solo set de entrenamiento para manejar mejor los datos.

```{r}
setwd('C:/Users/elosasso/OneDrive - Universidad Torcuato Di Tella/Mineria de datos/datasets')

train_data <- load_csv_data('train_4.csv', sample_ratio = 0.8)

#train_data[, Label := factor(ifelse(Label_max_played_dsi == 3,
#                                  "churn", "no_churn"))]

train_data[, Label := as.numeric(Label_max_played_dsi == 3)]

train_filtered <- filter(train_data, !(Label_max_played_dsi == 3 & install_date %in% (c(383:395))))
train_filtered$random <- rnorm(nrow(train_filtered), mean = 15)
```

Antes de avanzar sobre la separaci√≥n del dataset en conjuntos de train, validation & test se evaluan que variables podran llegar a ser relevantes o no para el modelo.


Como se habia visto anteriormente, las variables de age y site tienen un alto porcentaje de valores faltantes, y no es posible imputarlos de ninguna forma, por lo que se desconsideran para el modelo. Esto es lamentable en el caso de age, ya que probablemente (no lo sabemos), podria haber sido un predictor importante.

Por otro lado, el resto de las columnas con valores nulos se decide imputarles la media. Excepto por device_model, que se decidio eliminar los registros nulos porque son muy pocos y no afectar√° al modelo significativamente. Tambien se elimniaran los registros nulos de country porque, si bien podria llegar a ser una columna importante, solamente estariamos borrando el 2% de los registros

Tambien se vio que la columna BuyCard_sum_dsi_1 tiene valores negativos! Si bien esto se puede interpretar como que se le 'regalo' una carta, el hecho de que en las otras columnas de BuyCard_sum_dsi no hubiese valores negativos, lleva a pensar que son valores erroneos. Tomamos la decision de convertir estos valores negativos a 0, ya que es lo mas comun que observamos en las otras (entendemos el error que esto puede traer, pero es mejor que descartar completamente esta columna)

```{r}
# Reemplazo valores negativos de la columna BuyCard_sum_dsi1 con cero

train_filtered$BuyCard_sum_dsi1[train_filtered$BuyCard_sum_dsi1 < 0] <- 0
# Ecluyo las columnas age y site de mi dataframe

train_filtered <- select(train_filtered, -c(age, site))

# Elimnio los registros NA en la columna device_model

train_filtered <- filter(train_filtered, complete.cases(device_model))

# Imputo la media para el resto de las columnas con nulos

na_cols <- colnames(as.data.frame(find_nulls(select_if(train_filtered, is.numeric))))
for (i in na_cols){

    train_filtered[is.na(train_filtered[,i]),i] <- round(mean(train_filtered[,i], na.rm = TRUE))  
}

# Elimnio los registros NA en la columna country

train_filtered <- filter(train_filtered, complete.cases(country))
```


Se toma la decision de desconsiderar la columna categorical_7 porque entre las 7 columnas categorical, se juntan muchas variables categoricas como para poder manejarlas en el modelo, terminarian quedando muchas columnas. Por eso en esta primera etapa no se tomar√°n en cuenta.

```{r}
train_filtered <- select(train_filtered, -c(categorical_1,categorical_2,categorical_3,categorical_4,
                                           categorical_5,categorical_6,categorical_7,user_id, id,Label_max_played_dsi))
```                                  

Verifico que todas las variables tengan el type correcto

```{r}
sapply(train_filtered, class)
```


Se ve que la variable target es numerica y deberia ser un factor, y existen valores numericos en algunas variables como OpenChest_sum_dsi2, que deberian ser int (sino trae valores con coma, esto no tiene sentido)

```{r}
train_filtered <- transform(train_filtered,
                            OpenChest_sum_dsi2 = as.integer(OpenChest_sum_dsi2),
                            TutorialStart = as.factor(TutorialStart),
                            TutorialStartPart1 = as.factor(TutorialStartPart1),
                            TutorialStartPart2 = as.factor(TutorialStartPart2),
                            TutorialStartPart3 = as.factor(TutorialStartPart3),
                            TutorialStartPart4 = as.factor(TutorialStartPart4),
                            TutorialStartPart5 = as.factor(TutorialStartPart5),
                            TutorialStartPart6 = as.factor(TutorialStartPart6),
                            TutorialFinish = as.factor(TutorialFinish),
                            StartBattle_sum_dsi1 = as.integer(StartBattle_sum_dsi1),
                            ChangeArena_sum_dsi3 = as.integer(ChangeArena_sum_dsi3),
                            BuyCard_sum_dsi1 = as.integer(BuyCard_sum_dsi1)

                            )
```


# Separo en conjunto de train y validacion

Voy a separar en dos conjuntos para entrenar y validar posteriormente, siempre trabajando con el dataset de train_4.

```{r}

set.seed(123)
val_index <- sample(c(1:nrow(train_filtered)), nrow(train_filtered)*0.2)

train_data_tr <- train_filtered[-val_index,]
val_data <- train_filtered[val_index,]

```


# Modelado

Ahora aplico distintos modelos y preprocesamientos, validando contra el conjunto de validacion para ver si cualquier modificacion que haga esta mejorando o empeorando el modelo.

# Dataset crudo con un random forest 

### Busco eliminar variables innecesarias (todavia genero nuevas ni elimine outliers)


```{r}

rf <- randomForest(ntree = 100,
                   importance = TRUE,
                   Label ~ .,
                   data = select(val_data, -c(country,device_model))
  
)
```

```{r}
rf
```


```{r}

# Creo un dataframe con la importancia de cada variable

important_feautres <- as.data.frame(importance(rf))

important_feautres$features <- as.factor(rownames(important_feautres))

# Cambio el nombre de una columna

#names(important_feautres)[3] <- 'relevance'

# Grafico

options(repr.plot.width = 14, repr.plot.height = 25)
ggplot(important_feautres, aes(y = reorder(features, MeanDecreaseAccuracy), x = MeanDecreaseAccuracy)) + geom_bar(stat = "identity")

#plot(importance(rf))
```

Me quedo con las variables que tienen una importancia superior a la random

```{r}

#not_important_features <- c(select(filter(important_feautres, MeanDecreaseAccuracy < important_feautres$MeanDecreaseAccuracy[important_feautres$features == 'random']),features))

not_important_features <- c('LoseTournamentBattle_sum_dsi2','LoseTournamentBattle_sum_dsi1','LoseTournamentBattle_sum_dsi0','StartTournamentBattle_sum_dsi1','LoseTournamentBattle_sum_dsi3','StartTournamentBattle_sum_dsi0','WinTournamentBattle_sum_dsi3','QuitTournament_sum_dsi3','WinTournamentBattle_sum_dsi2','QuitTournament_sum_dsi0','JoinTournament_sum_dsi1','StartTournamentBattle_sum_dsi3','WinTournamentBattle_sum_dsi0','OpenPiggyBank_sum_dsi3','traffic_type','JoinTournament_sum_dsi2','JoinTournament_sum_dsi0','OpenPiggyBank_sum_dsi2','QuitTournament_sum_dsi1','QuitTournament_sum_dsi2','WinTournamentBattle_sum_dsi1','StartTournamentBattle_sum_dsi2','country','device_model','random','JoinTournament_sum_dsi3','TutorialStart','install_date','TutorialStartPart1','OpenPiggyBank_sum_dsi1','TutorialStartPart3','StartGameplayModeBattle_sum_dsi0','TutorialStartPart5','TutorialFinish','ChangeArena_sum_dsi3','OpenPiggyBank_sum_dsi0','TutorialStartPart6','StartGameplayModeBattle_sum_dsi1','BuyCard_sum_dsi3','StartGameplayModeBattle_sum_dsi3','TutorialStartPart4','TutorialStartPart2','platform','StartGameplayModeBattle_sum_dsi2','BuyCard_sum_dsi2') #volver a meter la random
```


# Modelo 1

Pruebo de nuevo el random forest solo con lo mas relevante (tarda mucho)


```{r}
rf <- randomForest(ntree = 100,
                   Label ~ .,
                   data = select(train_data_tr, -not_important_features))
```

```{r}

rf

```

# Modelo 2

Probando un modelo super sencillo de un solo arbol (pruebo sobre todo el data train, ya no sobre el conjunto de validacion porque ya elimine las variables menos descriptivas)


```{r}

fitControl <- trainControl(method = "cv",
                           number = 5,
                           verboseIter = TRUE,
                           classProbs=TRUE,
                           summaryFunction = twoClassSummary)


rpart_fit_grid <- train(x = train_data_tr %>% select(c(-not_important_features, - Label)),
                        y = train_data_tr$Label,
                        method = "rpart",
                        trControl = fitControl,
                        preProcess = c('center','scale'),
                        na.action = "na.omit",
                        metric = "ROC")

```

```{r}
rpart_fit_grid$results[which.max(rpart_fit_grid$results$ROC),]
```

Pruebo sobre el conjunto de validacion


```{r}

# Predecimos sobre los datos de evaluaci√≥n
eval_preds <- data.frame(Label = predict(rpart_fit_grid,
                                         select(val_data,-c(not_important_features,Label))))

```

Muestro matriz de confusion

```{r}
eval_preds$Real <- val_data$Label

table(predicho = eval_preds$Label, real = eval_preds$Real)

```

# Pruebo XGBOOST

```{r}
train_data_tr <- select(train_data_tr, -c(not_important_features))
val_data <- select(val_data, -c(not_important_features))


```


```{r}




dtrain <- xgb.DMatrix(data = as.matrix(train_data_tr[,colnames(train_data_tr) != 'Label']),
                      label = as.matrix(train_data_tr[,colnames(train_data_tr) == 'Label']))

dvalid <- xgb.DMatrix(data = as.matrix(val_data[,colnames(val_data) != 'Label']),
                      label = as.matrix(val_data[,colnames(val_data) == 'Label']))

## Entreno un modelo de xgboost con watchlist
watchlist <- list(train = dtrain, valid = dvalid)

vanilla_model <- xgb.train(data = dtrain, nrounds = 40,
                           watchlist = watchlist,
                           objective = "binary:logistic",  # Es la funci√≥n objetivo para clasificaci√≥n binaria
                           eval.metric = "auc",
                           print_every_n = 5)
```

Se obtiene una mejora en el auc general (recordemos que el modelo de arbol simple, con el recorte de features mas significativos, obtuvo un auc de 0.68). 

# Busqueda de hiperparametros

```{r funcion_sparse_xgboost_gridsearch}

random_grid <- function(size,
                        min_nrounds, max_nrounds,
                        min_max_depth, max_max_depth,
                        min_eta, max_eta,
                        min_gamma, max_gamma,
                        min_colsample_bytree, max_colsample_bytree,
                        min_min_child_weight, max_min_child_weight,
                        min_subsample, max_subsample) {

    rgrid <- data.frame(nrounds = if (min_nrounds == max_nrounds) {
                                      rep(min_nrounds, size)
                                  } else {
                                      sample(c(min_nrounds:max_nrounds),
                                             size = size, replace = TRUE)
                                  },
                        max_depth = if (min_max_depth == max_max_depth) {
                                      rep(min_max_depth, size)
                                  } else {
                                      sample(c(min_max_depth:max_max_depth),
                                             size = size, replace = TRUE)
                                  },
                        eta = if (min_eta == max_eta) {
                                      rep(min_eta, size)
                                  } else {
                                      round(runif(size, min_eta, max_eta), 5)
                                  },
                        gamma = if (min_gamma == max_gamma) {
                                      rep(min_gamma, size)
                                  } else {
                                      round(runif(size, min_gamma, max_gamma), 5)
                                  },
                        colsample_bytree = if (min_colsample_bytree == max_colsample_bytree) {
                                      rep(min_colsample_bytree, size)
                                  } else {
                                      round(runif(size, min_colsample_bytree, max_colsample_bytree), 5)
                                  },
                        min_child_weight = if (min_min_child_weight == max_min_child_weight) {
                                      rep(min_min_child_weight, size)
                                  } else {
                                      round(runif(size, min_min_child_weight, max_min_child_weight), 5)
                                  },
                        subsample = if (min_subsample == max_subsample) {
                                      rep(min_subsample, size)
                                  } else {
                                      round(runif(size, min_subsample, max_subsample), 5)
                                  })

    return(rgrid)
}


train_xgboost <- function(data_train, data_val, rgrid) {

    watchlist <- list(train = data_train, valid = data_val)

    predicted_models <- list()

    for (i in seq_len(nrow(rgrid))) {
        print(i)
        print(rgrid[i,])

        trained_model <- xgb.train(data = data_train,
                                   params=as.list(rgrid[i, c("max_depth",
                                                             "eta",
                                                             "gamma",
                                                             "colsample_bytree",
                                                             "subsample",
                                                             "min_child_weight")]),
                                   nrounds = rgrid[i, "nrounds"],
                                   watchlist = watchlist,
                                   objective = "binary:logistic",
                                   eval.metric = "auc",
                                   print_every_n = 10)

        perf_tr <- tail(trained_model$evaluation_log, 1)$train_auc
        perf_vd <- tail(trained_model$evaluation_log, 1)$valid_auc
        print(c(perf_tr, perf_vd))

        predicted_models[[i]] <- list(results = data.frame(rgrid[i,],
                                                           perf_tr = perf_tr,
                                                           perf_vd = perf_vd),
                                      model = trained_model)

        rm(trained_model)

        gc()
    }

    return(predicted_models)
}


result_table <- function(pred_models) {

    res_table <- data.frame()
    i <- 1

    for (m in pred_models) {
        res_table <- rbind(res_table, data.frame(i = i, m$results))
        i <- i + 1
    }

    res_table <- res_table[order(-res_table$perf_vd),]

    return(res_table)
}

```


```{r}
dtrain <- xgb.DMatrix(data = as.matrix(train_data_tr[,colnames(train_data_tr) != 'Label']),
                      label = as.matrix(train_data_tr[,colnames(train_data_tr) == 'Label']))

dvalid <- xgb.DMatrix(data = as.matrix(val_data[,colnames(val_data) != 'Label']),
                      label = as.matrix(val_data[,colnames(val_data) == 'Label']))

rgrid <- random_grid(size = 30,
                     min_nrounds = 20, max_nrounds = 60,
                     min_max_depth = 1, max_max_depth = 6,
                     min_eta = 0.0025, max_eta = 0.1,
                     min_gamma = 0, max_gamma = 1,
                     min_colsample_bytree = 0.6, max_colsample_bytree = 1,
                     min_min_child_weight = 1, max_min_child_weight = 10,
                     min_subsample = 0.75, max_subsample = 1)

predicted_models <- train_xgboost(dtrain, dvalid, rgrid)
```

```{r}
res_table <- result_table(predicted_models)
print(res_table)

```

```{r}
# Predicts in evaluation
eval_preds <- data.frame(id = 0:(nrow(evaluation)-1),
                         Label = predict(predicted_models[[res_table[1, "i"]]]$model,
                                         newdata = as.matrix(evaluation)))

options(scipen = 999)  # Para evitar que se guarden valores en formato cient√???fico
write.table(eval_preds, "modelo_con_random_search.csv",
            sep = ",", row.names = FALSE, quote = FALSE)
options(scipen=0, digits=7)  # Para volver al comportamiento tradicional

```


Esto sugiere que el xgboost es un buen modelo para seguir trabajando y, si bien podrÌan tocarse algunos hiperpar·metros, optamos por empezar a hacer una limpieza y preprocesamiento de los datos, a saber:

* Limpieza de outliers
* Sacar variables correlacionadas
* Normalizar los datos
* Obtener nuevas variables que correlacionen mejor con la variable target

# Limpieza de outliers

Visualizo las variables mas relevantes para ver los outliers de estos 






# Pruebo XGBOOST sobre el dataset limpio

```{r}
dtrain <- xgb.DMatrix(data = as.matrix(train_data_tr[,colnames(train_data_tr) != 'Label']),
                      label = as.matrix(train_data_tr[,colnames(train_data_tr) == 'Label']))

dvalid <- xgb.DMatrix(data = as.matrix(val_data[,colnames(val_data) != 'Label']),
                      label = as.matrix(val_data[,colnames(val_data) == 'Label']))

## Entreno un modelo de xgboost con watchlist
watchlist <- list(train = dtrain, valid = dvalid)

vanilla_model_v2 <- ?xgb.train(data = dtrain, nrounds = 40,
                           watchlist = watchlist,
                           objective = "binary:logistic",  # Es la funci√≥n objetivo para clasificaci√≥n binaria
                           eval.metric = "auc",
                           print_every_n = 5)

```

Falta seguir sacando outliers



# Pendiente: Usar el estimador MCD (Mean Covariance Determinant) para eliminar outliers de forma robusta


# Hago prediccion sobre el conjunto de evaluacion



```{r}
csv_dir <- "C:/Users/elosasso/OneDrive - Universidad Torcuato Di Tella/Mineria de datos/datasets/evaluation.csv"

evaluation <- load_csv_data(csv_dir, sample_ratio = 1)


evaluation <- transform(evaluation,
                            OpenChest_sum_dsi2 = as.integer(OpenChest_sum_dsi2),
                            TutorialStart = as.factor(TutorialStart),
                            TutorialStartPart1 = as.factor(TutorialStartPart1),
                            TutorialStartPart2 = as.factor(TutorialStartPart2),
                            TutorialStartPart3 = as.factor(TutorialStartPart3),
                            TutorialStartPart4 = as.factor(TutorialStartPart4),
                            TutorialStartPart5 = as.factor(TutorialStartPart5),
                            TutorialStartPart6 = as.factor(TutorialStartPart6),
                            TutorialFinish = as.factor(TutorialFinish),
                            StartBattle_sum_dsi1 = as.integer(StartBattle_sum_dsi1),
                            ChangeArena_sum_dsi3 = as.integer(ChangeArena_sum_dsi3),
                            BuyCard_sum_dsi1 = as.integer(BuyCard_sum_dsi1)

                            )
```

Me quedo con las mismas columnas que use en el entrenamiento

```{r}
selected_columns <- colnames(select(train_data_tr, -c(Label)))

evaluation <- select(evaluation,selected_columns)

```


```{r}
# Predecimos sobre los datos de evaluaci√≥n

eval_preds <- data.frame(id = 0:(nrow(evaluation)-1),
                         Label = predict(vanilla_model,
                                         as.matrix(evaluation)))

# Armo el archivo para subir a Kaggle
options(scipen = 999)  # Para evitar que se guarden valores en formato cient√???fico
write.table(eval_preds, "modelo_4_xgboost.csv",
            sep = ",", row.names = FALSE)
options(scipen=0, digits=7)  # Para volver al comportamiento tradicional

```

Quiero fijarme si hay valores nulos para imputar

```{r}

colSums(is.na(evaluation))[colSums(is.na(evaluation)) > 0]
```
```{r}

# Imputo la media para el resto de las columnas con nulos


evaluation$ChangeArena_sum_dsi3[is.na(evaluation$ChangeArena_sum_dsi3)] <- 0
evaluation$OpenChest_sum_dsi2[is.na(evaluation$OpenChest_sum_dsi2)] <- 4
evaluation$StartBattle_sum_dsi1[is.na(evaluation$StartBattle_sum_dsi1)] <- 13

#evaluation[is.na(evaluation[,ChangeArena_sum_dsi3]),ChangeArena_sum_dsi3]  #round(mean(evaluation[,ChangeArena_sum_dsi3], na.rm = TRUE))

#evaluation[is.na(evaluation[,OpenChest_sum_dsi2]),OpenChest_sum_dsi2] <- 4 #round(mean(evaluation[,OpenChest_sum_dsi2], na.rm = TRUE))

#evaluation[is.na(evaluation[,StartBattle_sum_dsi1]),StartBattle_sum_dsi1] <- 13 #round(mean(evaluation[,StartBattle_sum_dsi1], na.rm = TRUE))

```

```{r}

colSums(is.na(evaluation))[colSums(is.na(evaluation)) > 0]
```

Convierto la variable platform en una variable numerica

```{r}
evaluation <- select(dummy_cols(evaluation), -c(platform))
```

Aplico mi mejor modelo al conjunto validacion
```{r}
dim(evaluation); dim(train_data)

```

```{r}
holdout_index  <- sample(c(1:nrow(train_data)), 200000)
train_data_reduced <- train_data[-holdout_index,]
ts_pred <- knn(select(train_data_reduced,-c(Label,Label_max_played_dsi)), evaluation,
              train_data_reduced$Label, 30, prob = TRUE)
```
