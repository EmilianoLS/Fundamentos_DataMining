---
title: "Tp_churn_data_mining"
output: html_notebook
---

Importo las librerias que voy a usar

```{r message=FALSE, warning=FALSE}
library("stringr")
library("data.table")
library(ggplot2)
library(dplyr)
library(gridExtra)
library(reshape2)
library(stats)
library(corrplot)
library(class)
library(fastDummies)
library(rpart)
library(caret)
library(rpart.plot)
library(randomForest)
library(caTools)
library(robustbase)
library(xgboost)
```

Defino funcion para cargar todos los data frames de entrenamiento

```{r Load_csv_function, message=FALSE, warning=FALSE}
load_csv_data <- function(csv_file, sample_ratio = 1, drop_cols = NULL,
                          sel_cols = NULL) {
  # Esta funcion carga un unico csv con los parametros que le indique
  
  dt <- fread(csv_file, header = TRUE, sep = ",", stringsAsFactors = TRUE,
              na.strings = "", drop = drop_cols, select = sel_cols,
              showProgress = TRUE)
  return(dt)
}



load_train_data <- function(data_dir, train_file="/train_", sample_ratio=1,
                            drop_cols=NULL, sel_cols=NULL) {
  # Esta funcion se encarga de concatenar todos los dataframes juntos
  
  
  
  train_days <- seq(1, 5, by=1)
  
  dfs <- list()
  
  for (i in train_days){
  
    dfs[[i]] <- load_csv_data(csv_file = paste(data_dir,train_file,as.character(i),".csv", sep = ''), 
                              sample_ratio = sample_ratio, drop_cols = drop_cols, sel_cols = sel_cols)
  }
  
  # Uno todos los dataframes en uno solo
  
  df <- (rbindlist(dfs, fill=TRUE))
  
  # Reordeno las columnas alfabeticamente
  
  setcolorder(df, sort(colnames(df)))
  
  # Creo la columna Label con las condiciones de churn explicadas
  
  df[, Label := as.numeric(Label_max_played_dsi == 3)] 
  set.seed(123)
  if (sample_ratio < 1) {
    
    sample_size <- as.integer(sample_ratio * nrow(df))
    
    df <- df[sample(.N, sample_size)]
  }
  
  rm(dfs)
  
  
  return(df)
}
```

# Analisis exploratorio de las variables

En esta sección se va a hacer una primera aproximación a los datos, observando las relaciones entre las variables, entendiendo su composición, cómo están distribuidas, etc.

Cargo dataframe en su totalidad

```{r}
csv_dir <- "C:/Users/elosasso/OneDrive - Universidad Torcuato Di Tella/Mineria de datos/datasets"

train <- load_train_data(csv_dir, sample_ratio = 1)
train$churn <- train$Label == 1
```

Hago una primera visualización de la estructura para ver cómo se encuentran organizados los datos y para ver que todo esté cargado correctamente

```{r}
head((train));dim(train)
```

Visualizo cuantiles, mínimos, máximos y NAs

```{r}
summary(train)
```

A raíz de la tabla anterior, ya podemos observar dos cosas muy importantes:

* La columna 'age' no puede tomarse para analizar por su alto contenido de NAs, no podemos inferir estos datos de ninguna forma

* La columna 'BuyCard_sum_dsi1' tiene valores negativos, esto pareciera ser un error, y viendo el comportamiento en las demas columnas de este estilo, podria imputarse el valor 0 a estos negativos

Reviso las columnas con valores nulos y sus porcentajes

Para eso creo una funcion que me trae el porcentaje de nulos en las columnas que unicamente tienen nulos

```{r find_nulls_function, message=FALSE, warning=FALSE}
find_nulls <- function(df){
    columnas_nas <- list()

    for (i in colnames(df)){
      nans <- nrow(filter(select(df,i), !complete.cases(select(df,i))))

      if (nans != 0){

        columnas_nas[[i]] <- (nans/nrow(df))*100
      }

    }
    return(columnas_nas)
    }

as.data.frame(find_nulls(train))
```

# Visualización

En esta parte, empezamos a meternos más en los datos. Queremos entender, o al menos, tener un primer insight de las variables que más relación tienen con la target, si hay mayor o menor proporción de alguna variable en particular.

La primera relación que se nos ocurrió fue con las variables relacionadas al tutorial. Cuando jugamos el juego, vimos que el tutorial es bastante extenso, y cubre bastantes de los elementos más importantes del juego. Suponemos que existe, entonces, alguna relación entre el hecho de haber terminado el tutorial y hacer churn o no.

```{r}
ggplot(train) + geom_bar(mapping = aes(x = TutorialFinish, fill = churn), position = 'fill') +
labs(x = 'Termino el tutorial', y = '% churn') +
ggtitle('Proporcion de churn segun termino o no el tutorial')
```

Vemos en el gráfico de proporciones que, si bien la proporción de personas que hicieron churn es mayor para el grupo que no terminó el tutorial, la diferencia no es significativa respecto a los que sí lo terminaron, sino que tienen niveles similares.


Por otro lado, supuonemos que una persona que pierda más batallas será más propensa a hacer Churn. 
Para comprobar esta hipótesis, sumamos todas las batallas iniciadas, y todas las batallas perdidas, y hacemos un gráfico de relación, separado por los grupos de churn y no churn.


```{r}
train$sum_lost_battles <- train$LoseBattle_sum_dsi0 + train$LoseBattle_sum_dsi1 + train$LoseBattle_sum_dsi2 + train$LoseBattle_sum_dsi3
train$sum_win_battles <- train$WinBattle_sum_dsi0 + train$WinBattle_sum_dsi1 + train$WinBattle_sum_dsi2 + train$WinBattle_sum_dsi3
train$sum_start_battle <- train$StartBattle_sum_dsi0 + train$StartBattle_sum_dsi1 + train$StartBattle_sum_dsi2 + train$StartBattle_sum_dsi3

# Tomo una muestra del 10% del total para graficar los puntos porque sino es muy pesado

set.seed(123)
sample_size <- as.integer(0.1 * nrow(train))    
df <- train[sample(.N, sample_size)]

ggplot(filter(df, complete.cases(sum_start_battle))) +
geom_point(mapping = aes(x = sum_start_battle, y = sum_lost_battles, color = churn), alpha = 0.3) +
geom_smooth(mapping = aes(x = sum_start_battle, y = sum_lost_battles)) +
labs(x = 'Batallas iniciadas', y = 'Batallas perdidas') +
ggtitle('Batallas perdidas vs Batallas iniciadas')
```

Lo primero que vemos es que hay una relación casi lineal entre las batallas iniciadas y las perdidas. Esto es lógico, porque cuanto más se juegue, es lógico pensar que la cantidad de veces que pierda también tienda a aumentar. 

Sin embargo, cuando vemos la distribución de los puntos de cada color, vemos que todos se encuentran bastante superpuestos, con lo cual, estas variables no separan casi nada los grupos de churn y no churn.

Otra idea, relacionada a las batallas iniciadas del punto anterior, es que puede llegar a ocurrir que exista una relación entre la cantidad de sesiones que inicia el usuario y el hecho de hacer churn. Por esto, vemos la cantidad de sesiones iniciadas para cada grupo, en cada uno de los días desde la instalación.

Luego se calcula la media de sesiones iniciadas para entender si verdaderamente hay una relación.

```{r}
train_melt <- melt(select(train, user_id, StartSession_sum_dsi0,StartSession_sum_dsi1,StartSession_sum_dsi2,
                          StartSession_sum_dsi3, churn), id = c('user_id','churn'))

grafico <- train_melt %>% group_by(variable,churn) %>% summarize(total = sum(value)) %>% ungroup()
grafico

ggplot(grafico, mapping = aes(x = total, y = variable, fill = churn)) +
geom_bar(color = 'black',stat = 'identity', position = 'dodge') + facet_wrap(~churn, ncol = 1) +
labs(x = 'Total de sesiones', y = 'Dia') +
ggtitle('Cantidad de sesiones por dia')
```

Agrupo por dia desde la instalacion y calculo el intervalo de confianza para la media

```{r}
grafico_2 <- train_melt %>% group_by(variable,churn) %>%
summarize(intervalo_inf = t.test(value, conf.level = 0.95)$conf.int[1], 
         intervalo_sup = t.test(value, conf.level = 0.95)$conf.int[2],
         mean = mean(value)) %>% ungroup()

grafico_2

ggplot(grafico_2) +
geom_point(mapping = aes(x = mean, y = variable, fill = churn),color = 'black',stat = 'identity') + 
geom_errorbar(aes(xmin = intervalo_inf, xmax = intervalo_sup, y = variable), color = 'red') +
facet_wrap(~churn, ncol = 1) +
labs(x = 'Media de sesiones', y = 'Dia') + 
ggtitle('Cantidad de sesiones promedio por dia')
```

Vemos que encontramos alguna especie de relación entre las sesiones y los grupos de churn y no churn. Puntualmente se ve que el comportamiento en sesiones iniciadas para un grupo y otro es completamente diferente. 

Por un lado, el grupo de usuarios que hicieron churn tiene una media de sesiones iniciadas diarias más baja que el grupo que no hizo churn, y que posteriormente baja hasta el tercer día de la instalación. Esto no pasa con el otro grupo, sino que más bien se mantiene constante durante los primeros días


```{r}


train$install_day <- if_else(train$install_date %in% seq(from = 0, to = (56*7), by = 7), 'A', 
                             if_else(train$install_date %in% seq(from = 1, to = (56*7)+1, by = 7), 'B',
                                     if_else(train$install_date %in% seq(from = 2, to = (56*7)+2, by = 7), 'C',
                                             if_else(train$install_date %in% seq(from = 3, to = (56*7)+3, by = 7), 'D',
                                                     if_else(train$install_date %in% seq(from = 4, to = (56*7)+4, by = 7), 'F', 
                                                             if_else(train$install_date %in% seq(from = 5, to = (55*7)+5, by = 7), 'G',
                                                                     if_else(train$install_date %in% seq(from = 6, to = (55*7)+6, by = 7), 'H','NA')))))))
#train$install_day <- if_else(train$install_date+1 %in% c(1:56*7,by = 7))

```

```{r}
prop_churn_dia <- as.data.frame(prop.table(table(train$install_day, train$Label),2)[,2])

names(prop_churn_dia)[1] <- 'Proporcion_churn'
prop_churn_dia$day <- row.names(prop_churn_dia)
ggplot(prop_churn_dia, aes(day, Proporcion_churn)) + geom_point() +
  labs(x = 'Dia de instalacion', title = 'Proporcion de churn total segun el dia de instalacion')
rm(prop_churn_dia)
```




Busco si existe una relaciÃ³n entre la cantidad de batallas ganadas/ batallas perdidas

```{r}
train$rt_lose_win_0 <- train$WinBattle_sum_dsi0 - train$LoseBattle_sum_dsi0
train$rt_lose_win_1 <- train$WinBattle_sum_dsi1 - train$LoseBattle_sum_dsi1
train$rt_lose_win_2 <- train$WinBattle_sum_dsi2 - train$LoseBattle_sum_dsi2
train$rt_lose_win_3 <- train$WinBattle_sum_dsi3 - train$LoseBattle_sum_dsi3

train_melt <- melt(select(train, user_id, rt_lose_win_0,rt_lose_win_1,rt_lose_win_2,
                          rt_lose_win_3, churn), id = c('user_id','churn'))

grafico_4 <- train_melt %>% group_by(variable,churn) %>%
summarize(intervalo_inf = t.test(value, conf.level = 0.95)$conf.int[1], 
         intervalo_sup = t.test(value, conf.level = 0.95)$conf.int[2],
         mean = mean(value)) %>% ungroup()

ggplot(grafico_4) +
geom_point(mapping = aes(x = mean, y = variable, fill = churn),color = 'black',stat = 'identity') + 
geom_errorbar(aes(xmin = intervalo_inf, xmax = intervalo_sup, y = variable), color = 'red') +
facet_wrap(~churn, ncol = 1) +
labs(x = 'Media de resultados por dia', y = 'Dia') + 
ggtitle('Resultados promedio por dÃ???a')
```

Promedio de batallas perdidas por dÃ???a

```{r}
train_melt <- melt(select(train, user_id, LoseBattle_sum_dsi0,LoseBattle_sum_dsi1,LoseBattle_sum_dsi2,
                          LoseBattle_sum_dsi3, churn), id = c('user_id','churn'))

grafico_5 <- train_melt %>% group_by(variable,churn) %>%
summarize(intervalo_inf = t.test(value, conf.level = 0.95)$conf.int[1], 
         intervalo_sup = t.test(value, conf.level = 0.95)$conf.int[2],
         mean = mean(value)) %>% ungroup()

ggplot(grafico_5) +
geom_point(mapping = aes(x = mean, y = variable, fill = churn),color = 'black',stat = 'identity') + 
geom_errorbar(aes(xmin = intervalo_inf, xmax = intervalo_sup, y = variable), color = 'red') +
facet_wrap(~churn, ncol = 1) +
labs(x = 'Media de batallas perdidas', y = 'Dia') + 
ggtitle('Batallas perdidas en promedio por dÃ???a')
```

Promedio de batallas iniciadas (esto lo hago para entender si la menor cantidad de batallas perdidas por aquellos que hicieron churn en el grÃ¡fico anterior se debe a que jugaron menos)

```{r}
train_melt <- melt(select(filter(train,complete.cases(StartBattle_sum_dsi1)), user_id,
                          StartBattle_sum_dsi0,StartBattle_sum_dsi1,StartBattle_sum_dsi2,
                          StartBattle_sum_dsi3, churn), id = c('user_id','churn'))

grafico_6 <- train_melt %>% group_by(variable,churn) %>%
summarize(intervalo_inf = t.test(value, conf.level = 0.95)$conf.int[1], 
         intervalo_sup = t.test(value, conf.level = 0.95)$conf.int[2],
         mean = mean(value)) %>% ungroup()

ggplot(grafico_6) +
geom_point(mapping = aes(x = mean, y = variable, fill = churn),color = 'black',stat = 'identity') + 
geom_errorbar(aes(xmin = intervalo_inf, xmax = intervalo_sup, y = variable), color = 'red') +
facet_wrap(~churn, ncol = 1) +
labs(x = 'Media de batallas inicidadas', y = 'Dia') + 
ggtitle('Batallas iniciadas en promedio por dÃ???a')
```

Feature Engineering
-------------------

Ahora que ya preparamos una analitica y comprendemos un poco mejor los datos, procedemos a preparar las variables para poder aplicarles un modelo.

Primero tengo que excluir los casos no confiables: Todos aquellos que tengan Label_max_played_dsi igual a 3, que hayan sido instalados en los dias que van de 383 a 395 inclusive, ya que no son validos (desconozco si la persona hizo churn o no).

Entreno con un solo set de entrenamiento para manejar mejor los datos.

```{r}
setwd('C:/Users/elosasso/OneDrive - Universidad Torcuato Di Tella/Mineria de datos/datasets')

train_data <- load_csv_data('train_4.csv', sample_ratio = 0.8)

#train_data[, Label := factor(ifelse(Label_max_played_dsi == 3,
#                                  "churn", "no_churn"))]

train_data[, Label := as.numeric(Label_max_played_dsi == 3)]

train_filtered <- filter(train_data, !(Label_max_played_dsi == 3 & install_date %in% (c(383:395))))
train_filtered$random <- rnorm(nrow(train_filtered), mean = 15)
```

Antes de avanzar sobre la separaciÃ³n del dataset en conjuntos de train, validation & test se evaluan que variables podran llegar a ser relevantes o no para el modelo.


Como se habia visto anteriormente, las variables de age y site tienen un alto porcentaje de valores faltantes, y no es posible imputarlos de ninguna forma, por lo que se desconsideran para el modelo. Esto es lamentable en el caso de age, ya que probablemente (no lo sabemos), podria haber sido un predictor importante.

Por otro lado, el resto de las columnas con valores nulos se decide imputarles la media. Excepto por device_model, que se decidio eliminar los registros nulos porque son muy pocos y no afectarÃ¡ al modelo significativamente. Tambien se elimniaran los registros nulos de country porque, si bien podria llegar a ser una columna importante, solamente estariamos borrando el 2% de los registros

Tambien se vio que la columna BuyCard_sum_dsi_1 tiene valores negativos! Si bien esto se puede interpretar como que se le 'regalo' una carta, el hecho de que en las otras columnas de BuyCard_sum_dsi no hubiese valores negativos, lleva a pensar que son valores erroneos. Tomamos la decision de convertir estos valores negativos a 0, ya que es lo mas comun que observamos en las otras (entendemos el error que esto puede traer, pero es mejor que descartar completamente esta columna)

```{r}
# Reemplazo valores negativos de la columna BuyCard_sum_dsi1 con cero

train_filtered$BuyCard_sum_dsi1[train_filtered$BuyCard_sum_dsi1 < 0] <- 0
# Ecluyo las columnas age y site de mi dataframe

train_filtered <- select(train_filtered, -c(age, site))

# Elimnio los registros NA en la columna device_model

train_filtered <- filter(train_filtered, complete.cases(device_model))

# Imputo la media para el resto de las columnas con nulos

na_cols <- colnames(as.data.frame(find_nulls(select_if(train_filtered, is.numeric))))
for (i in na_cols){

    train_filtered[is.na(train_filtered[,i]),i] <- round(mean(train_filtered[,i], na.rm = TRUE))  
}

# Elimnio los registros NA en la columna country

train_filtered <- filter(train_filtered, complete.cases(country))
```


Se toma la decision de desconsiderar la columna categorical_7 porque entre las 7 columnas categorical, se juntan muchas variables categoricas como para poder manejarlas en el modelo, terminarian quedando muchas columnas. Por eso en esta primera etapa no se tomarÃ¡n en cuenta.

```{r}
train_filtered <- select(train_filtered, -c(categorical_1,categorical_2,categorical_3,categorical_4,
                                           categorical_5,categorical_6,categorical_7,user_id, id,Label_max_played_dsi))
```                                  

Verifico que todas las variables tengan el type correcto

```{r}
sapply(train_filtered, class)
```


Se ve que la variable target es numerica y deberia ser un factor, y existen valores numericos en algunas variables como OpenChest_sum_dsi2, que deberian ser int (sino trae valores con coma, esto no tiene sentido)

```{r}
train_filtered <- transform(train_filtered,
                            OpenChest_sum_dsi2 = as.integer(OpenChest_sum_dsi2),
                            TutorialStart = as.factor(TutorialStart),
                            TutorialStartPart1 = as.factor(TutorialStartPart1),
                            TutorialStartPart2 = as.factor(TutorialStartPart2),
                            TutorialStartPart3 = as.factor(TutorialStartPart3),
                            TutorialStartPart4 = as.factor(TutorialStartPart4),
                            TutorialStartPart5 = as.factor(TutorialStartPart5),
                            TutorialStartPart6 = as.factor(TutorialStartPart6),
                            TutorialFinish = as.factor(TutorialFinish),
                            StartBattle_sum_dsi1 = as.integer(StartBattle_sum_dsi1),
                            ChangeArena_sum_dsi3 = as.integer(ChangeArena_sum_dsi3),
                            BuyCard_sum_dsi1 = as.integer(BuyCard_sum_dsi1)

                            )
```


# Separo en conjunto de train y validacion

Voy a separar en dos conjuntos para entrenar y validar posteriormente, siempre trabajando con el dataset de train_4.

```{r}

set.seed(123)
val_index <- sample(c(1:nrow(train_filtered)), nrow(train_filtered)*0.2)

train_data_tr <- train_filtered[-val_index,]
val_data <- train_filtered[val_index,]

```


# Modelado

Ahora aplico distintos modelos y preprocesamientos, validando contra el conjunto de validacion para ver si cualquier modificacion que haga esta mejorando o empeorando el modelo.

# Dataset crudo con un random forest 

### Busco eliminar variables innecesarias (todavia genero nuevas ni elimine outliers)


```{r}

rf <- randomForest(ntree = 100,
                   importance = TRUE,
                   Label ~ .,
                   data = select(val_data, -c(country,device_model))
  
)
```

```{r}
rf
```


```{r}

# Creo un dataframe con la importancia de cada variable

important_feautres <- as.data.frame(importance(rf))

important_feautres$features <- as.factor(rownames(important_feautres))

# Cambio el nombre de una columna

#names(important_feautres)[3] <- 'relevance'

# Grafico

options(repr.plot.width = 14, repr.plot.height = 25)
ggplot(important_feautres, aes(y = reorder(features, MeanDecreaseAccuracy), x = MeanDecreaseAccuracy)) + geom_bar(stat = "identity")

#plot(importance(rf))
```

Me quedo con las variables que tienen una importancia superior a la random

```{r}

#not_important_features <- c(select(filter(important_feautres, MeanDecreaseAccuracy < important_feautres$MeanDecreaseAccuracy[important_feautres$features == 'random']),features))

not_important_features <- c('LoseTournamentBattle_sum_dsi2','LoseTournamentBattle_sum_dsi1','LoseTournamentBattle_sum_dsi0','StartTournamentBattle_sum_dsi1','LoseTournamentBattle_sum_dsi3','StartTournamentBattle_sum_dsi0','WinTournamentBattle_sum_dsi3','QuitTournament_sum_dsi3','WinTournamentBattle_sum_dsi2','QuitTournament_sum_dsi0','JoinTournament_sum_dsi1','StartTournamentBattle_sum_dsi3','WinTournamentBattle_sum_dsi0','OpenPiggyBank_sum_dsi3','traffic_type','JoinTournament_sum_dsi2','JoinTournament_sum_dsi0','OpenPiggyBank_sum_dsi2','QuitTournament_sum_dsi1','QuitTournament_sum_dsi2','WinTournamentBattle_sum_dsi1','StartTournamentBattle_sum_dsi2','country','device_model','random','JoinTournament_sum_dsi3','TutorialStart','install_date','TutorialStartPart1','OpenPiggyBank_sum_dsi1','TutorialStartPart3','StartGameplayModeBattle_sum_dsi0','TutorialStartPart5','TutorialFinish','ChangeArena_sum_dsi3','OpenPiggyBank_sum_dsi0','TutorialStartPart6','StartGameplayModeBattle_sum_dsi1','BuyCard_sum_dsi3','StartGameplayModeBattle_sum_dsi3','TutorialStartPart4','TutorialStartPart2','platform','StartGameplayModeBattle_sum_dsi2','BuyCard_sum_dsi2') #volver a meter la random
```


# Modelo 1

Pruebo de nuevo el random forest solo con lo mas relevante (tarda mucho)


```{r}
rf <- randomForest(ntree = 100,
                   Label ~ .,
                   data = select(train_data_tr, -not_important_features))
```

```{r}

rf

```

# Modelo 2

Probando un modelo super sencillo de un solo arbol (pruebo sobre todo el data train, ya no sobre el conjunto de validacion porque ya elimine las variables menos descriptivas)


```{r}

fitControl <- trainControl(method = "cv",
                           number = 5,
                           verboseIter = TRUE,
                           classProbs=TRUE,
                           summaryFunction = twoClassSummary)


rpart_fit_grid <- train(x = train_data_tr %>% select(c(-not_important_features, - Label)),
                        y = train_data_tr$Label,
                        method = "rpart",
                        trControl = fitControl,
                        preProcess = c('center','scale'),
                        na.action = "na.omit",
                        metric = "ROC")

```

```{r}
rpart_fit_grid$results[which.max(rpart_fit_grid$results$ROC),]
```

Pruebo sobre el conjunto de validacion


```{r}

# Predecimos sobre los datos de evaluaciÃ³n
eval_preds <- data.frame(Label = predict(rpart_fit_grid,
                                         select(val_data,-c(not_important_features,Label))))

```

Muestro matriz de confusion

```{r}
eval_preds$Real <- val_data$Label

table(predicho = eval_preds$Label, real = eval_preds$Real)

```

# Pruebo XGBOOST

```{r}
train_data_tr <- select(train_data_tr, -c(not_important_features))
val_data <- select(val_data, -c(not_important_features))


```


```{r}




dtrain <- xgb.DMatrix(data = as.matrix(train_data_tr[,colnames(train_data_tr) != 'Label']),
                      label = as.matrix(train_data_tr[,colnames(train_data_tr) == 'Label']))

dvalid <- xgb.DMatrix(data = as.matrix(val_data[,colnames(val_data) != 'Label']),
                      label = as.matrix(val_data[,colnames(val_data) == 'Label']))

## Entreno un modelo de xgboost con watchlist
watchlist <- list(train = dtrain, valid = dvalid)

vanilla_model <- xgb.train(data = dtrain, nrounds = 40,
                           watchlist = watchlist,
                           objective = "binary:logistic",  # Es la funciÃ³n objetivo para clasificaciÃ³n binaria
                           eval.metric = "auc",
                           print_every_n = 5)
```

Se obtiene una mejora en el auc general (recordemos que el modelo de arbol simple, con el recorte de features mas significativos, obtuvo un auc de 0.68). 

# Busqueda de hiperparametros

```{r funcion_sparse_xgboost_gridsearch}

random_grid <- function(size,
                        min_nrounds, max_nrounds,
                        min_max_depth, max_max_depth,
                        min_eta, max_eta,
                        min_gamma, max_gamma,
                        min_colsample_bytree, max_colsample_bytree,
                        min_min_child_weight, max_min_child_weight,
                        min_subsample, max_subsample) {

    rgrid <- data.frame(nrounds = if (min_nrounds == max_nrounds) {
                                      rep(min_nrounds, size)
                                  } else {
                                      sample(c(min_nrounds:max_nrounds),
                                             size = size, replace = TRUE)
                                  },
                        max_depth = if (min_max_depth == max_max_depth) {
                                      rep(min_max_depth, size)
                                  } else {
                                      sample(c(min_max_depth:max_max_depth),
                                             size = size, replace = TRUE)
                                  },
                        eta = if (min_eta == max_eta) {
                                      rep(min_eta, size)
                                  } else {
                                      round(runif(size, min_eta, max_eta), 5)
                                  },
                        gamma = if (min_gamma == max_gamma) {
                                      rep(min_gamma, size)
                                  } else {
                                      round(runif(size, min_gamma, max_gamma), 5)
                                  },
                        colsample_bytree = if (min_colsample_bytree == max_colsample_bytree) {
                                      rep(min_colsample_bytree, size)
                                  } else {
                                      round(runif(size, min_colsample_bytree, max_colsample_bytree), 5)
                                  },
                        min_child_weight = if (min_min_child_weight == max_min_child_weight) {
                                      rep(min_min_child_weight, size)
                                  } else {
                                      round(runif(size, min_min_child_weight, max_min_child_weight), 5)
                                  },
                        subsample = if (min_subsample == max_subsample) {
                                      rep(min_subsample, size)
                                  } else {
                                      round(runif(size, min_subsample, max_subsample), 5)
                                  })

    return(rgrid)
}


train_xgboost <- function(data_train, data_val, rgrid) {

    watchlist <- list(train = data_train, valid = data_val)

    predicted_models <- list()

    for (i in seq_len(nrow(rgrid))) {
        print(i)
        print(rgrid[i,])

        trained_model <- xgb.train(data = data_train,
                                   params=as.list(rgrid[i, c("max_depth",
                                                             "eta",
                                                             "gamma",
                                                             "colsample_bytree",
                                                             "subsample",
                                                             "min_child_weight")]),
                                   nrounds = rgrid[i, "nrounds"],
                                   watchlist = watchlist,
                                   objective = "binary:logistic",
                                   eval.metric = "auc",
                                   print_every_n = 10)

        perf_tr <- tail(trained_model$evaluation_log, 1)$train_auc
        perf_vd <- tail(trained_model$evaluation_log, 1)$valid_auc
        print(c(perf_tr, perf_vd))

        predicted_models[[i]] <- list(results = data.frame(rgrid[i,],
                                                           perf_tr = perf_tr,
                                                           perf_vd = perf_vd),
                                      model = trained_model)

        rm(trained_model)

        gc()
    }

    return(predicted_models)
}


result_table <- function(pred_models) {

    res_table <- data.frame()
    i <- 1

    for (m in pred_models) {
        res_table <- rbind(res_table, data.frame(i = i, m$results))
        i <- i + 1
    }

    res_table <- res_table[order(-res_table$perf_vd),]

    return(res_table)
}

```


```{r}
dtrain <- xgb.DMatrix(data = as.matrix(train_data_tr[,colnames(train_data_tr) != 'Label']),
                      label = as.matrix(train_data_tr[,colnames(train_data_tr) == 'Label']))

dvalid <- xgb.DMatrix(data = as.matrix(val_data[,colnames(val_data) != 'Label']),
                      label = as.matrix(val_data[,colnames(val_data) == 'Label']))

rgrid <- random_grid(size = 30,
                     min_nrounds = 20, max_nrounds = 60,
                     min_max_depth = 1, max_max_depth = 6,
                     min_eta = 0.0025, max_eta = 0.1,
                     min_gamma = 0, max_gamma = 1,
                     min_colsample_bytree = 0.6, max_colsample_bytree = 1,
                     min_min_child_weight = 1, max_min_child_weight = 10,
                     min_subsample = 0.75, max_subsample = 1)

predicted_models <- train_xgboost(dtrain, dvalid, rgrid)
```

```{r}
res_table <- result_table(predicted_models)
print(res_table)

```

```{r}
# Predicts in evaluation
eval_preds <- data.frame(id = 0:(nrow(evaluation)-1),
                         Label = predict(predicted_models[[res_table[1, "i"]]]$model,
                                         newdata = as.matrix(evaluation)))

options(scipen = 999)  # Para evitar que se guarden valores en formato cientÃ???fico
write.table(eval_preds, "modelo_con_random_search.csv",
            sep = ",", row.names = FALSE, quote = FALSE)
options(scipen=0, digits=7)  # Para volver al comportamiento tradicional

```


Esto sugiere que el xgboost es un buen modelo para seguir trabajando y, si bien podrían tocarse algunos hiperparámetros, optamos por empezar a hacer una limpieza y preprocesamiento de los datos, a saber:

* Limpieza de outliers
* Sacar variables correlacionadas
* Normalizar los datos
* Obtener nuevas variables que correlacionen mejor con la variable target

# Limpieza de outliers

Visualizo las variables mas relevantes para ver los outliers de estos 






# Pruebo XGBOOST sobre el dataset limpio

```{r}
dtrain <- xgb.DMatrix(data = as.matrix(train_data_tr[,colnames(train_data_tr) != 'Label']),
                      label = as.matrix(train_data_tr[,colnames(train_data_tr) == 'Label']))

dvalid <- xgb.DMatrix(data = as.matrix(val_data[,colnames(val_data) != 'Label']),
                      label = as.matrix(val_data[,colnames(val_data) == 'Label']))

## Entreno un modelo de xgboost con watchlist
watchlist <- list(train = dtrain, valid = dvalid)

vanilla_model_v2 <- ?xgb.train(data = dtrain, nrounds = 40,
                           watchlist = watchlist,
                           objective = "binary:logistic",  # Es la funciÃ³n objetivo para clasificaciÃ³n binaria
                           eval.metric = "auc",
                           print_every_n = 5)

```

Falta seguir sacando outliers



# Pendiente: Usar el estimador MCD (Mean Covariance Determinant) para eliminar outliers de forma robusta


# Hago prediccion sobre el conjunto de evaluacion



```{r}
csv_dir <- "C:/Users/elosasso/OneDrive - Universidad Torcuato Di Tella/Mineria de datos/datasets/evaluation.csv"

evaluation <- load_csv_data(csv_dir, sample_ratio = 1)


evaluation <- transform(evaluation,
                            OpenChest_sum_dsi2 = as.integer(OpenChest_sum_dsi2),
                            TutorialStart = as.factor(TutorialStart),
                            TutorialStartPart1 = as.factor(TutorialStartPart1),
                            TutorialStartPart2 = as.factor(TutorialStartPart2),
                            TutorialStartPart3 = as.factor(TutorialStartPart3),
                            TutorialStartPart4 = as.factor(TutorialStartPart4),
                            TutorialStartPart5 = as.factor(TutorialStartPart5),
                            TutorialStartPart6 = as.factor(TutorialStartPart6),
                            TutorialFinish = as.factor(TutorialFinish),
                            StartBattle_sum_dsi1 = as.integer(StartBattle_sum_dsi1),
                            ChangeArena_sum_dsi3 = as.integer(ChangeArena_sum_dsi3),
                            BuyCard_sum_dsi1 = as.integer(BuyCard_sum_dsi1)

                            )
```

Me quedo con las mismas columnas que use en el entrenamiento

```{r}
selected_columns <- colnames(select(train_data_tr, -c(Label)))

evaluation <- select(evaluation,selected_columns)

```


```{r}
# Predecimos sobre los datos de evaluaciÃ³n

eval_preds <- data.frame(id = 0:(nrow(evaluation)-1),
                         Label = predict(vanilla_model,
                                         as.matrix(evaluation)))

# Armo el archivo para subir a Kaggle
options(scipen = 999)  # Para evitar que se guarden valores en formato cientÃ???fico
write.table(eval_preds, "modelo_4_xgboost.csv",
            sep = ",", row.names = FALSE)
options(scipen=0, digits=7)  # Para volver al comportamiento tradicional

```

Quiero fijarme si hay valores nulos para imputar

```{r}

colSums(is.na(evaluation))[colSums(is.na(evaluation)) > 0]
```
```{r}

# Imputo la media para el resto de las columnas con nulos


evaluation$ChangeArena_sum_dsi3[is.na(evaluation$ChangeArena_sum_dsi3)] <- 0
evaluation$OpenChest_sum_dsi2[is.na(evaluation$OpenChest_sum_dsi2)] <- 4
evaluation$StartBattle_sum_dsi1[is.na(evaluation$StartBattle_sum_dsi1)] <- 13

#evaluation[is.na(evaluation[,ChangeArena_sum_dsi3]),ChangeArena_sum_dsi3]  #round(mean(evaluation[,ChangeArena_sum_dsi3], na.rm = TRUE))

#evaluation[is.na(evaluation[,OpenChest_sum_dsi2]),OpenChest_sum_dsi2] <- 4 #round(mean(evaluation[,OpenChest_sum_dsi2], na.rm = TRUE))

#evaluation[is.na(evaluation[,StartBattle_sum_dsi1]),StartBattle_sum_dsi1] <- 13 #round(mean(evaluation[,StartBattle_sum_dsi1], na.rm = TRUE))

```

```{r}

colSums(is.na(evaluation))[colSums(is.na(evaluation)) > 0]
```

Convierto la variable platform en una variable numerica

```{r}
evaluation <- select(dummy_cols(evaluation), -c(platform))
```

Aplico mi mejor modelo al conjunto validacion
```{r}
dim(evaluation); dim(train_data)

```

```{r}
holdout_index  <- sample(c(1:nrow(train_data)), 200000)
train_data_reduced <- train_data[-holdout_index,]
ts_pred <- knn(select(train_data_reduced,-c(Label,Label_max_played_dsi)), evaluation,
              train_data_reduced$Label, 30, prob = TRUE)
```
